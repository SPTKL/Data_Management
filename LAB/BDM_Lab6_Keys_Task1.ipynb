{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.39.235:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're creating two RDD, one is from the README file of Spark\n",
    "# and the other is directly from a list within the notebook.\n",
    "#\n",
    "# If you downloaded Spark, the README file is in the same folder\n",
    "# as the one you extracted. If you use other package management\n",
    "# methods like 'brew', 'dnf', 'apt', etc. you will need to figure\n",
    "# out the path of Spark by printing sys.path.\n",
    "\n",
    "rdd = sc.textFile('C:\\spark-2.3.1-bin-hadoop2.6\\README.md')\n",
    "testRdd = sc.parallelize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['#', 'Apache', 'Spark'], [], ['Spark', 'is', 'a', 'fast', 'and', 'general', 'cluster', 'computing', 'system', 'for', 'Big', 'Data.', 'It', 'provides']]\n",
      "\n",
      "['#', 'Apache', 'Spark']\n"
     ]
    }
   ],
   "source": [
    "# Here, we're showing the difference between map() and flatMap()\n",
    "# doing the line split and get back the first 3 elements, take(3).\n",
    "# - map() is a one-to-one mapping, just like Python, so the first\n",
    "# line prints out 3 lists, each consists words per each.\n",
    "# - flatMap() is a one-to-many mapping, like MapReduce's map(). So\n",
    "# the second line prints out only 3 words.\n",
    "\n",
    "wordsPerLine = rdd.map(lambda line: line.split()).take(3)\n",
    "words = rdd.flatMap(lambda line: line.split()).take(3)\n",
    "\n",
    "print ('%s\\n\\n%s' % (wordsPerLine, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1), ('is', 6)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the word count example with Spark using the approach\n",
    "# shown in the slides, i.e. staying true to the MapReduce paradigm.\n",
    "# Note that groupByKey() will sort and group everything together by\n",
    "# keys first. Then the function in mapValues() will each get applied\n",
    "# per each (key, list of values) pair. This could be an issue if we\n",
    "# have a pair with lots of values since all of the values have to be\n",
    "# stored in memory.\n",
    "\n",
    "wc = rdd.flatMap(lambda line: line.split()) \\\n",
    "        .map(lambda x: (x.lower(), 1)) \\\n",
    "        .groupByKey() \\\n",
    "        .mapValues(lambda values: sum(values))\n",
    "wc.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1), ('is', 6)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is another approach with reduceByKey() instead of groupByKey().\n",
    "# The reduce function provided for reduceByKey() only takes 2 params\n",
    "# at a time, thus, doesn't suffer the scalability issue. It also has\n",
    "# better benefits in term of parallelism.\n",
    "\n",
    "wc = rdd.flatMap(lambda line: line.split()) \\\n",
    "        .map(lambda x: (x.lower(), 1)) \\\n",
    "        .reduceByKey(lambda x,y: x+y)\n",
    "wc.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 25), ('to', 19), ('spark', 16)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we'd like to compute the top 3 most popular words in Spark. We\n",
    "# can use the RDD's top() function directly. This is much easier\n",
    "# than the two-step MapReduce job, where we had to first compute the\n",
    "# top 3 words per partition, then another top 3 on top of that. In\n",
    "# fact, this is exactly how Spark RDD's top() function is implemented.\n",
    "# More info can be found here:\n",
    "# https://github.com/apache/spark/blob/master/python/pyspark/rdd.py#L1249\n",
    "wc.top(3, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB 6 - Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAT_FN = 'SAT_Results.csv'\n",
    "HSD_FN = 'DOE_High_School_Directory_2014-2015.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is a way to read CSV file from within Spark directly into a \n",
    "# Spark's DataFrame, which we will not be covering yet. Just putting\n",
    "# it here so that we have a reference for now. Note that, the \n",
    "# 'parserLib' option is important for reading multi-line fields of CSV.\n",
    "df = spark.read \\\n",
    "            .format(\"com.databricks.spark.csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .option(\"parserLib\", \"UNIVOCITY\") \\\n",
    "            .load(HSD_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dbn', 'string'),\n",
       " ('school_name', 'string'),\n",
       " ('boro', 'string'),\n",
       " ('building_code', 'string'),\n",
       " ('phone_number', 'string'),\n",
       " ('fax_number', 'string'),\n",
       " ('grade_span_min', 'string'),\n",
       " ('grade_span_max', 'int'),\n",
       " ('expgrade_span_min', 'string'),\n",
       " ('expgrade_span_max', 'int'),\n",
       " ('bus', 'string'),\n",
       " ('subway', 'string'),\n",
       " ('primary_address_line_1', 'string'),\n",
       " ('city', 'string'),\n",
       " ('state_code', 'string'),\n",
       " ('zip', 'int'),\n",
       " ('website', 'string'),\n",
       " ('total_students', 'string'),\n",
       " ('campus_name', 'string'),\n",
       " ('school_type', 'string'),\n",
       " ('overview_paragraph', 'string'),\n",
       " ('program_highlights', 'string'),\n",
       " ('language_classes', 'string'),\n",
       " ('advancedplacement_courses', 'string'),\n",
       " ('online_ap_courses', 'string'),\n",
       " ('online_language_courses', 'string'),\n",
       " ('extracurricular_activities', 'string'),\n",
       " ('psal_sports_boys', 'string'),\n",
       " ('psal_sports_girls', 'string'),\n",
       " ('psal_sports_coed', 'string'),\n",
       " ('school_sports', 'string'),\n",
       " ('partner_cbo', 'string'),\n",
       " ('partner_hospital', 'string'),\n",
       " ('partner_highered', 'string'),\n",
       " ('partner_cultural', 'string'),\n",
       " ('partner_nonprofit', 'string'),\n",
       " ('partner_corporate', 'string'),\n",
       " ('partner_financial', 'string'),\n",
       " ('partner_other', 'string'),\n",
       " ('addtl_info1', 'string'),\n",
       " ('addtl_info2', 'string'),\n",
       " ('start_time', 'string'),\n",
       " ('end_time', 'string'),\n",
       " ('se_services', 'string'),\n",
       " ('ell_programs', 'string'),\n",
       " ('school_accessibility_description', 'string'),\n",
       " ('number_programs', 'int'),\n",
       " ('priority01', 'string'),\n",
       " ('priority02', 'string'),\n",
       " ('priority03', 'string'),\n",
       " ('priority04', 'string'),\n",
       " ('priority05', 'string'),\n",
       " ('priority06', 'string'),\n",
       " ('priority07', 'string'),\n",
       " ('priority08', 'string'),\n",
       " ('priority09', 'string'),\n",
       " ('priority10', 'string'),\n",
       " ('Location 1', 'string')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'DBN'),\n",
       " (1, 'SCHOOL NAME'),\n",
       " (2, 'Num of SAT Test Takers'),\n",
       " (3, 'SAT Critical Reading Avg. Score'),\n",
       " (4, 'SAT Math Avg. Score'),\n",
       " (5, 'SAT Writing Avg. Score')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We read the SAT score to our RDD. Note that the use_unicode can be\n",
    "# changed accordingly to your data file to handle Unicode. If you cannot\n",
    "# parse your data due to an 'utf8' or 'ascii' decoding issue, it might\n",
    "# be a good thing to try flipping the use_unicode parameter here.\n",
    "\n",
    "sat = sc.textFile(SAT_FN, use_unicode=True).cache()\n",
    "\n",
    "# This line for us to list the column index and column names to see\n",
    "# which column we need to use for our task. In this case, we're\n",
    "# interested in the number of test takers (#2) and the math score (#4).\n",
    "list(enumerate(sat.first().split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBN,SCHOOL NAME,Num of SAT Test Takers,SAT Critical Reading Avg. Score,SAT Math Avg. Score,SAT Writing Avg. Score\n",
      "02M047,47 THE AMERICAN SIGN LANGUAGE AND ENGLISH SECONDARY SCHOOL,16,395,400,387\n"
     ]
    }
   ],
   "source": [
    "# Note that, our data input includes a header line that we don't want to\n",
    "# use in analysis. We can remove the header line from our RDD by doing\n",
    "# a 'filter' to remove all rows that matches the header like below. Though\n",
    "# this works, it means that we have to apply the filter function on *all*\n",
    "# row, which could be a lot of computation.\n",
    "\n",
    "noHeaderRDD = sat.filter(lambda x: not x.startswith('DBN,SCHOOL'))\n",
    "print (sat.first())\n",
    "print (noHeaderRDD.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('02M047', (6400, 16)),\n",
       " ('21K410', (207575, 475)),\n",
       " ('30Q301', (43120, 98)),\n",
       " ('17K382', (22066, 59)),\n",
       " ('18K637', (13335, 35))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternatively, we can perform the header checking per-partition, instead\n",
    "# of per-row like below. mapPartitions() is another type of map operators\n",
    "# in Spark that is similar to Hadoop Streaming's map(). It is many-to-many.\n",
    "# RDD in Spark are divided into partitions (as we read or as provided by\n",
    "# HDFS), each partition can be processed in parallel using a function\n",
    "# supplied to the mapPartitions() call.\n",
    "# \n",
    "# In addition to mapPartitions(), Spark also provides a variation called\n",
    "# mapPartitionsWithIndex() that provides information on which partition\n",
    "# we are currently processing. Indeed, mapPartitionsWithIndex() is the\n",
    "# the operator with the lowest overhead (since mapPartitions() get mapped\n",
    "# to mapPartitionsWithIndex) and also the most efficient one among all the\n",
    "# map operators.\n",
    "#\n",
    "# So our logic below is to use the partition index to check if we're hitting\n",
    "# the header (aka the first partition). If so, we just skip the first row.\n",
    "\n",
    "def extractScores(partId, records):\n",
    "    if partId==0:\n",
    "        next(records)\n",
    "    import csv\n",
    "    reader = csv.reader(records)\n",
    "    for row in reader:\n",
    "        if row[2]!='s': # to filter our bad-quality data\n",
    "            (dbn,takers,score) = (row[0], int(row[2]), int(row[4]))\n",
    "            yield (dbn, (score*takers, takers))\n",
    "\n",
    "satScores = sat.mapPartitionsWithIndex(extractScores)\n",
    "satScores.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'dbn'),\n",
       " (1, 'school_name'),\n",
       " (2, 'boro'),\n",
       " (3, 'building_code'),\n",
       " (4, 'phone_number'),\n",
       " (5, 'fax_number'),\n",
       " (6, 'grade_span_min'),\n",
       " (7, 'grade_span_max'),\n",
       " (8, 'expgrade_span_min'),\n",
       " (9, 'expgrade_span_max'),\n",
       " (10, 'bus'),\n",
       " (11, 'subway'),\n",
       " (12, 'primary_address_line_1'),\n",
       " (13, 'city'),\n",
       " (14, 'state_code'),\n",
       " (15, 'zip'),\n",
       " (16, 'website'),\n",
       " (17, 'total_students'),\n",
       " (18, 'campus_name'),\n",
       " (19, 'school_type'),\n",
       " (20, 'overview_paragraph'),\n",
       " (21, 'program_highlights'),\n",
       " (22, 'language_classes'),\n",
       " (23, 'advancedplacement_courses'),\n",
       " (24, 'online_ap_courses'),\n",
       " (25, 'online_language_courses'),\n",
       " (26, 'extracurricular_activities'),\n",
       " (27, 'psal_sports_boys'),\n",
       " (28, 'psal_sports_girls'),\n",
       " (29, 'psal_sports_coed'),\n",
       " (30, 'school_sports'),\n",
       " (31, 'partner_cbo'),\n",
       " (32, 'partner_hospital'),\n",
       " (33, 'partner_highered'),\n",
       " (34, 'partner_cultural'),\n",
       " (35, 'partner_nonprofit'),\n",
       " (36, 'partner_corporate'),\n",
       " (37, 'partner_financial'),\n",
       " (38, 'partner_other'),\n",
       " (39, 'addtl_info1'),\n",
       " (40, 'addtl_info2'),\n",
       " (41, 'start_time'),\n",
       " (42, 'end_time'),\n",
       " (43, 'se_services'),\n",
       " (44, 'ell_programs'),\n",
       " (45, 'school_accessibility_description'),\n",
       " (46, 'number_programs'),\n",
       " (47, 'priority01'),\n",
       " (48, 'priority02'),\n",
       " (49, 'priority03'),\n",
       " (50, 'priority04'),\n",
       " (51, 'priority05'),\n",
       " (52, 'priority06'),\n",
       " (53, 'priority07'),\n",
       " (54, 'priority08'),\n",
       " (55, 'priority09'),\n",
       " (56, 'priority10'),\n",
       " (57, 'Location 1')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we do the same thing with the school directory data\n",
    "schools = sc.textFile(HSD_FN, use_unicode=True).cache()\n",
    "list(enumerate(schools.first().split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSchools(partId, list_of_records):\n",
    "    if partId==0: \n",
    "        next(list_of_records) # skipping the first line\n",
    "    import csv\n",
    "    reader = csv.reader(list_of_records)\n",
    "    for row in reader:\n",
    "        if len(row)==58 and row[17].isdigit():\n",
    "            (dbn, boro, total_students) = (row[0], row[2], int(row[17]))\n",
    "            if total_students>500: # filter to keep the large schools\n",
    "                yield (dbn, boro)\n",
    "\n",
    "largeSchools = schools.mapPartitionsWithIndex(extractSchools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = largeSchools.join(satScores).values() \\\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])) \\\n",
    "    .mapValues(lambda x: x[0]/x[1]) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bronx', 470.198606271777),\n",
       " ('Staten Island', 477.9099864130435),\n",
       " ('Manhattan', 514.9312780989081),\n",
       " ('Brooklyn', 487.46256168204246),\n",
       " ('Queens', 474.3679400475233)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "input: \n",
    "(school, [line]) * (school, score) -> (school, ([line], score))\n",
    "                                   -> [([line)], score)]\n",
    "(school, [score])\n",
    "output: \n",
    "(line, avg_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowLen = len(schools.first().split(','))\n",
    "\n",
    "def extractSchools2(partId, list_of_records):\n",
    "    if partId==0: \n",
    "        next(list_of_records) # skipping the first line\n",
    "    import csv\n",
    "    reader = csv.reader(list_of_records)\n",
    "    for row in reader:\n",
    "        if len(row)==rowLen:\n",
    "            (dbn, boro, total_students) = (row[0], row[10], int(row[11]))\n",
    "            lines = buses.split(', ') + ', '.join([direction.split(' to ')[0]\n",
    "                                                  for direction in subways.split(' ; ')]).split(', ')\n",
    "            yield (dbn, boro)\n",
    "\n",
    "schoolWithLines = schools.mapPartitionsWithIndex(extractSchools2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[64] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schoolWithLines.join(satScores)\\\n",
    "    .flatMap(lambda dbn_states: [(lines, dbn_state[1][1]) for line in dbn_states[1][0]])\n",
    "    .reduceByKey()\n",
    "    .mapValues\n",
    "    ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
